{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS4zj/zvC9ZH/6htYyuW6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvslopes/Fluxo-de-Dados-SDBE/blob/main/03_Construindo_C%C3%B3digos_ETL_com_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**01- Construindo os códigos ETL com a Linguagem Python**\n",
        "\n",
        "**Objetivo:**\\\n",
        "Após a modelagem do Data Warehouse, a criação das tabelas de dimensões e da tabela Fato, foi construído o processo de ETL - Extração, Transformação e Carregamento com a linguagem Python e o Apache Airflow.\n"
      ],
      "metadata": {
        "id": "h9LrqIpT95fH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.1- Primeiro Bloco de Código \" O processo de Imports\"**"
      ],
      "metadata": {
        "id": "3WiraH3hXX0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primeiro Bloco de comandos os \"Imports\"\n",
        "import csv\n",
        "import airflow\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from airflow.operators.postgres_operator import PostgresOperator\n",
        "from airflow.utils.dates import days_ago"
      ],
      "metadata": {
        "id": "2DZbC8rH-A9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Comentando os código do Pacote imports**\n",
        "\n",
        "* **import \"CSV\"**:\\\n",
        "Importa o módulo \"csv\" em Python. Nossa fonte de dados é um arquivo CSV. Portanto, foi necessário manipular esses arquivos em algum momento.\n",
        "\n",
        "* **import \"airflow\"**:\\\n",
        "importa o pacote \"airflow\" em Python pois foi necessário construir toda a DAG (Directed Acyclic Graph, ou Grafo Direcionado Acíclico), que será lida pelo Apache Airflow.\n",
        "\n",
        "* **import \"time\"**:\\\n",
        "Importa o módulo \"time\" em Python. fornece funcionalidades relacionadas ao tempo e à temporização.\n",
        "\n",
        "* **import pandas as pd**:\\\n",
        "Importa o módulo \"pandas\" em Python que foi renomeado para \"pd\" para facilitar referências posteriores. O \"pandas\" é uma biblioteca amplamente usada para manipulação e análise de dados. Forneceu estruturas de dados e funções para trabalhar com dados tabulares.\n",
        "\n",
        "**Em seguida, temos os imports específicos do Apache Airflow:**\n",
        "\n",
        "* **from datetime import datetime e from datetime import timedelta:**\\\n",
        " Importa as classes **datetime e timedelta** do módulo **datetime** para lidar com datas e horários.\n",
        "\n",
        "* **from airflow import DAG:**\\\n",
        " Importa a classe **DAG** do pacote **airflow**, que é fundamental para definir e configurar fluxos de trabalho no Apache Airflow.\n",
        "\n",
        "* **from airflow.operators.python_operator import PythonOperator:**\\\n",
        " Importa a classe **PythonOperator** do **pacote airflow.operators.python_operator**, que permite executar tarefas Python como parte de uma DAG.\n",
        "\n",
        "* **from airflow.operators.postgres_operator import PostgresOperator:**\\\n",
        " Importa a classe **PostgresOperator do pacote airflow.operators.postgres_operator**, que é usada para executar tarefas relacionadas ao PostgreSQL em uma DAG.\n",
        "\n",
        "* **from airflow.utils.dates import days_ago:**\\\n",
        " Importa a função **days_ago** do pacote **airflow.utils.dates**, que é útil para calcular datas com base na contagem regressiva de dias a partir da data atual."
      ],
      "metadata": {
        "id": "nXYPbpQsNf8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.2- Segundo Bloco de Código \" Criação da DAG - Directed Acyclic Graphs\"**\n",
        "###**Tópico Argumentos**"
      ],
      "metadata": {
        "id": "cxB4AXDMW_Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Argumentos\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'depends_on_past': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n"
      ],
      "metadata": {
        "id": "VRDvChFVX_u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comentando o código do Argumentos**\n",
        "\n",
        "**Argumentos:** representa um direcionário de argumentos\n",
        "\n",
        "* **| 'owner': 'airflow' |**\\\n",
        "Indica que o próprietario ou responsável pela DAG é o Airflow.\n",
        "\n",
        "* **| start_date': datetime(2023, 1, 1) |**\\\n",
        " Indica que a data e a hora que a DAG começará a ser agendada e executada. A DAG começará a ser executada a partir do primeiro dia de janeiro de 2023. A partir desse ponto, a DAG será agendada de acordo com o cronograma especificado, que pode ser definido usando as outras configurações, como **schedule_interval.**\n",
        "\n",
        " A data  foi retroativa **datetime(2023, 1, 1)**, pois quando criamos uma GAG, nos não conseguimos mais modificar esta data, dessa forma é sempre bom colocarmos uma data retroativa, para que comecemos a execuação no momento que começarmos a DAG.\n",
        "\n",
        "* **| 'depends_on_past': False' |**\\\n",
        "Indica que a execução atual da DAG não depende do sucesso da execução anterior. Ou seja, cada execução da DAG ocorrerá independentemente do resultado das execuções anteriores.\n",
        "\n",
        "* **| 'retries': 1 |**\\\n",
        "Indica que em caso de falha na execução da tarefa, haverá 01 tentativa de retomar a tarefa. Se a tarefa falhar novamente após essa tentativa, não haverá mais retentativas, e a execução será considerada como falha permanente.\n",
        "\n",
        "* **| 'retry_delay': timedelta(minutes=5) |**\\\n",
        "Indica que o intervalo de tempo que deve ser aguardado antes de tentar novamente uma tarefa que falhou. Neste caso, após uma falha, a tarefa será reagendada para uma nova tentativa após 5 minutos. Isso permite um atraso entre as tentativas para evitar sobrecarregar o sistema em caso de falhas frequentes.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rbOskjGpYLQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Tópico criação da DAG**"
      ],
      "metadata": {
        "id": "WDy7N2Gc65Xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar a DAG\n",
        "\n",
        "# https://crontab.guru/\n",
        "dag_log_solutions = DAG(dag_id = \"logsol\",\n",
        "                   default_args = default_args,\n",
        "                   schedule_interval = '0 0 * * *',\n",
        "                   dagrun_timeout = timedelta(minutes = 60),\n",
        "                   description = 'Job ETL de Carga no DW com Airflow',\n",
        "                   start_date = airflow.utils.dates.days_ago(1)\n",
        ")\n"
      ],
      "metadata": {
        "id": "VYx3GUsRdLhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comentando o código do Criar DAG**\n",
        "\n",
        "\n",
        "* **|dag_log_solutions = DAG(dag_id = \"logsol\"|**\\\n",
        "Indica que a DAG está sendo nomeada como \"dag_log_solutions_dsa\" com o ID \"logsol\".\n",
        "\n",
        "* **| default_args = default_args |**\\\n",
        "Indica a Definição doo argumento e o conteúdo desse argumento que foi a **lista de agumentos definida na etapa anterior.**\n",
        "\n",
        "* **| schedule_interval = '0 0 * * *' |**\n",
        "Indica a Definição do intervalo de agendamento: No caso de \"schedule_interval = '0 0 * * *'\", isso significa o seguinte:\n",
        "\n",
        " \"0 0 * * *\"é uma expressão cron que define a programação. Ela indica que a DAG será executada diariamente à meia-noite (00:00) todos os dias.\n",
        "Portanto, a DAG será agendada para ser executada uma vez por dia, sempre à meia-noite, de acordo com esse cronograma.\n",
        "\n",
        "* **| dagrun_timeout = timedelta(minutes = 60) |**\n",
        "\n",
        " Indica a Definição do tempo máximo permitido para a execução de uma instância da DAG (DAG run). Isso significa que cada execução da DAG tem um limite de tempo de 60 minutos (1 hora). Se a execução da DAG não for concluída dentro desse limite de tempo, ela será considerada falha.\n",
        "\n",
        "* **| description = 'Job ETL de Carga no DW com Airflow' |**\\\n",
        "Indica a Definição de uma descrição ou um resumo do que é o trabalho (job) realizado pela DAG.\n",
        "\n",
        "* **| start_date = airflow.utils.dates.days_ago(1) |**\\\n",
        "Indica que o start Date será um dia anterior a data que vamos criar no Airflow, ou seja  indica que a DAG começará a ser executada um dia atrás da data atual.\n",
        "Essa configuração permite agendar a DAG para ser executada a partir de um ponto no passado, o que pode ser útil em cenários em que você deseja retroativamente executar tarefas ou processar dados a partir de uma data anterior.\n",
        "\n",
        "**Obs:** Temos também um start_date no  default_args, irá gerar conflito? Não gerará conflito e se tivermos o argumentos em dois locais, vale por ultimo o que estiver a DAG\n",
        "\n",
        "**Dessa forma temos a DAG criada que é o bloco principal para execuação do Apahe Airflow.**\n",
        "\n"
      ],
      "metadata": {
        "id": "wxP55sglkQUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.3- Terceiro Bloco de Código \" Função para Extarir um Arquivo**"
      ],
      "metadata": {
        "id": "1BSxGQR08YTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "def func_carrega_dados_clientes():\n",
        "\n",
        "    # Obter o caminho do arquivo CSV\n",
        "    csv_file_path = '/opt/airflow/dags/dados/DIM_CLIENTE.csv'\n",
        "\n",
        "    # Inicializa o contador\n",
        "    i = 0\n",
        "\n",
        "    # Abrir o Arquivo CSV\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1\n",
        "            \n",
        "            # Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)\n",
        "\n",
        "            # Inserir dados na tabela PostgreSQL\n",
        "            sql_query_cli = \"INSERT INTO varejo.DIM_CLIENTE (%s) VALUES (%s)\" % (','.join(dados_cli.keys()), ','.join([item for item in dados_cli.values()]))\n",
        "    \n",
        "            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_clientes_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 params = (dados_cli),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "    \n",
        "            # Executa o operador\n",
        "            postgres_operator.execute()\n",
        "\n",
        "\n",
        "tarefa_carrega_dados_clientes = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_clientes',\n",
        "        python_callable = func_carrega_dados_clientes,\n",
        "        provide_context = True,\n",
        "        dag = dag_log_solutions\n",
        "    )\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "2-bciFH_WkdW"
      }
    }
  ]
}