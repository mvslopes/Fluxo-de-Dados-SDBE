{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWyvJIy31pCXwcI7JOPuqb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvslopes/Fluxo-de-Dados-SDBE/blob/main/03_Construindo_C%C3%B3digos_ETL_com_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**SPRINT 02- OBJETIVOS**\n",
        "\n",
        "O principal objetivo da Sprint 02 deste projeto é avançar na implementação do Data Warehouse, focando especificamente no desenvolvimento do processo de ETL (Extração, Transformação e Carregamento).\n",
        "\n",
        "Para atingir esse objetivo, a equipe se concentrará em construir os códigos em **Python e configurar o Apache Airflow** para automatizar e gerenciar as etapas do ETL.\n",
        "\n",
        "Os principais passos desta sprint incluem:\n",
        "\n",
        "**Desenvolvimento das Rotinas de Extração:** Será criado o código Python responsável por extrair os dados das fontes de origem. Isso garantirá que os dados sejam coletados de maneira eficiente e confiável.\n",
        "\n",
        "**Transformação dos Dados:** Ocrrerá as transformações necessárias nos dados extraídos. Isso pode incluir limpeza, formatação, agregação e enriquecimento dos dados para prepará-los para análise.\n",
        "\n",
        "**Carregamento no Data Warehouse:** Após a transformação, os dados serão carregados no Data Warehouse, onde serão armazenados de forma estruturada e organizada. **O Apache Airflow** será configurado para agendar e executar essas tarefas de carregamento de maneira automatizada e programada.\n",
        "\n",
        "\n",
        "Ao final desta sprint, espera-se que o processo de ETL esteja completamente implementado e funcional, permitindo que os dados sejam coletados, transformados e carregados no Data Warehouse de forma automatizada e confiável. Isso será essencial para a construção de um ambiente de análise de dados robusto e eficaz."
      ],
      "metadata": {
        "id": "bQVxJN92XFJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**01- Construindo os códigos ETL com a Linguagem Python**\n",
        "\n",
        "**Objetivo:**\\\n",
        "Após a modelagem do Data Warehouse, a criação das tabelas de dimensões e da tabela Fato, foi construído o processo de ETL - Extração, Transformação e Carregamento com a linguagem Python e o Apache Airflow.\n"
      ],
      "metadata": {
        "id": "h9LrqIpT95fH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.1- Primeiro Bloco de Código \" O processo de Imports\"**"
      ],
      "metadata": {
        "id": "3WiraH3hXX0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primeiro Bloco de comandos os \"Imports\"\n",
        "import csv\n",
        "import airflow\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from airflow.operators.postgres_operator import PostgresOperator\n",
        "from airflow.utils.dates import days_ago"
      ],
      "metadata": {
        "id": "2DZbC8rH-A9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Comentando os código do Pacote imports**\n",
        "\n",
        "* **| import \"CSV\"** | : Importa o módulo \"csv\" em Python. Nossa fonte de dados é um arquivo CSV. Portanto, foi necessário manipular esses arquivos em algum momento.\n",
        "\n",
        "* **| import \"airflow\" |** : importa o pacote \"airflow\" em Python pois foi necessário construir toda a DAG (Directed Acyclic Graph, ou Grafo Direcionado Acíclico), que será lida pelo Apache Airflow.\n",
        "\n",
        "* **| import \"time\" |** : Importa o módulo \"time\" em Python. fornece funcionalidades relacionadas ao tempo e à temporização.\n",
        "\n",
        "* **| import pandas as pd |** : Importa o módulo \"pandas\" em Python que foi renomeado para \"pd\" para facilitar referências posteriores. O \"pandas\" é uma biblioteca amplamente usada para manipulação e análise de dados. Forneceu estruturas de dados e funções para trabalhar com dados tabulares.\n",
        "\n",
        "**Em seguida, temos os imports específicos do Apache Airflow:**\n",
        "\n",
        "* **| from datetime import datetime | e | from datetime import timedelta | :**\\\n",
        " Importa as classes **datetime e timedelta** do módulo **datetime** para lidar com datas e horários.\n",
        "\n",
        "* **| from airflow import DAG | :** Importa a classe **DAG** do pacote **airflow**, que é fundamental para definir e configurar fluxos de trabalho no Apache Airflow.\n",
        "\n",
        "* **| from airflow.operators.python_operator import PythonOperator | :** Importa a classe **PythonOperator** do **pacote airflow.operators.python_operator**, que permite executar tarefas Python como parte de uma DAG.\n",
        "\n",
        "* **| from airflow.operators.postgres_operator import PostgresOperator | :** Importa a classe **PostgresOperator do pacote airflow.operators.postgres_operator**, que é usada para executar tarefas relacionadas ao PostgreSQL em uma DAG.\n",
        "\n",
        "* **| from airflow.utils.dates import days_ago |:** Importa a função **days_ago** do pacote **airflow.utils.dates**, que é útil para calcular datas com base na contagem regressiva de dias a partir da data atual.\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "nXYPbpQsNf8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.2- Segundo Bloco de Código \" Criação da DAG - Directed Acyclic Graphs\"**\n",
        "###**Tópico Argumentos**"
      ],
      "metadata": {
        "id": "cxB4AXDMW_Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Argumentos\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'depends_on_past': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n"
      ],
      "metadata": {
        "id": "VRDvChFVX_u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comentando o código do Argumentos**\n",
        "\n",
        "**Argumentos:** representa um direcionário de argumentos\n",
        "\n",
        "* **| 'owner': 'airflow' |** : Indica que o próprietario ou responsável pela DAG é o Airflow.\n",
        "\n",
        "* **| start_date': datetime(2023, 1, 1) |** : Indica que a data e a hora que a DAG começará a ser agendada e executada. A DAG começará a ser executada a partir do primeiro dia de janeiro de 2023. A partir desse ponto, a DAG será agendada de acordo com o cronograma especificado, que pode ser definido usando as outras configurações, como **schedule_interval.**\n",
        "\n",
        " A data  foi retroativa **datetime(2023, 1, 1)**, pois quando criamos uma GAG, nos não conseguimos mais modificar esta data, dessa forma é sempre bom colocarmos uma data retroativa, para que comecemos a execuação no momento que começarmos a DAG.\n",
        "\n",
        "* **| 'depends_on_past': False' |** : Indica que a execução atual da DAG não depende do sucesso da execução anterior. Ou seja, cada execução da DAG ocorrerá independentemente do resultado das execuções anteriores.\n",
        "\n",
        "* **| 'retries': 1 |** : Indica que em caso de falha na execução da tarefa, haverá 01 tentativa de retomar a tarefa. Se a tarefa falhar novamente após essa tentativa, não haverá mais retentativas, e a execução será considerada como falha permanente.\n",
        "\n",
        "* **| 'retry_delay': timedelta(minutes=5) |** : Indica que o intervalo de tempo que deve ser aguardado antes de tentar novamente uma tarefa que falhou. Neste caso, após uma falha, a tarefa será reagendada para uma nova tentativa após 5 minutos. Isso permite um atraso entre as tentativas para evitar sobrecarregar o sistema em caso de falhas frequentes.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rbOskjGpYLQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Tópico criação da DAG**"
      ],
      "metadata": {
        "id": "WDy7N2Gc65Xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar a DAG\n",
        "\n",
        "# https://crontab.guru/\n",
        "dag_log_solutions = DAG(dag_id = \"logsol\",\n",
        "                   default_args = default_args,\n",
        "                   schedule_interval = '0 0 * * *',\n",
        "                   dagrun_timeout = timedelta(minutes = 60),\n",
        "                   description = 'Job ETL de Carga no DW com Airflow',\n",
        "                   start_date = airflow.utils.dates.days_ago(1)\n",
        ")\n"
      ],
      "metadata": {
        "id": "VYx3GUsRdLhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comentando o código do Criar DAG**\n",
        "\n",
        "\n",
        "* **|dag_log_solutions = DAG(dag_id = \"logsol\"|** : Indica que a DAG está sendo nomeada como \"dag_log_solutions_dsa\" com o ID \"logsol\".\n",
        "\n",
        "* **| default_args = default_args |** : Indica a Definição doo argumento e o conteúdo desse argumento que foi a **lista de agumentos definida na etapa anterior.**\n",
        "\n",
        "* **| schedule_interval = '0 0 * * *' |** : Indica a Definição do intervalo de agendamento: No caso de \"schedule_interval = '0 0 * * *'\", isso significa o seguinte:\n",
        "\n",
        " \"0 0 * * *\"é uma expressão cron que define a programação. Ela indica que a DAG será executada diariamente à meia-noite (00:00) todos os dias.\n",
        "Portanto, a DAG será agendada para ser executada uma vez por dia, sempre à meia-noite, de acordo com esse cronograma.\n",
        "\n",
        "* **| dagrun_timeout = timedelta(minutes = 60) |** : Indica a Definição do tempo máximo permitido para a execução de uma instância da DAG (DAG run). Isso significa que cada execução da DAG tem um limite de tempo de 60 minutos (1 hora). Se a execução da DAG não for concluída dentro desse limite de tempo, ela será considerada falha.\n",
        "\n",
        "* **| description = 'Job ETL de Carga no DW com Airflow' |** : Indica a Definição de uma descrição ou um resumo do que é o trabalho (job) realizado pela DAG.\n",
        "\n",
        "* **| start_date = airflow.utils.dates.days_ago(1) |** : Indica que o start Date será um dia anterior a data que vamos criar no Airflow, ou seja  indica que a DAG começará a ser executada um dia atrás da data atual.\n",
        "Essa configuração permite agendar a DAG para ser executada a partir de um ponto no passado, o que pode ser útil em cenários em que você deseja retroativamente executar tarefas ou processar dados a partir de uma data anterior.\n",
        "\n",
        "**Obs:** Temos também um start_date no  default_args, irá gerar conflito? Não gerará conflito e se tivermos o argumentos em dois locais, vale por ultimo o que estiver a DAG\n",
        "\n",
        "**Dessa forma temos a DAG criada que é o bloco principal para execuação do Apahe Airflow.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wxP55sglkQUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1.3- Terceiro Bloco de Código \" Função para Carregar dados no DW**"
      ],
      "metadata": {
        "id": "1BSxGQR08YTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func_carrega_dados_clientes():\n",
        "\n",
        "    # Obter o caminho do arquivo CSV\n",
        "    csv_file_path = '/opt/airflow/dags/dados/DIM_CLIENTE.csv'\n",
        "\n",
        "    # Inicializa o contador\n",
        "    i = 0\n",
        "\n",
        "    # Abrir o Arquivo CSV\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1\n",
        "\n",
        "            # Extrai uma linha como dicionário\n",
        "            dados_cli = dict(item)\n",
        "\n",
        "            # Inserir dados na tabela PostgreSQL\n",
        "\n",
        "            sql_query_cli =\n",
        "\n",
        "            \"INSERT INTO varejo.DIM_CLIENTE (%s) VALUES (%s)\" %\n",
        "\n",
        "             (','.join(dados_cli.keys()),\n",
        "\n",
        "             ','.join([item for item in dados_cli.values()]))\n",
        "\n",
        "            # Operador do Postgres com incremento no id da tarefa\n",
        "            # (para cada linha inserida)\n",
        "\n",
        "            postgres_operator = PostgresOperator\n",
        "             (task_id = 'carrega_dados_clientes_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 params = (dados_cli),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "\n",
        "            # Executa o operador\n",
        "            postgres_operator.execute()\n",
        "\n",
        "\n",
        "tarefa_carrega_dados_clientes = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_clientes',\n",
        "        python_callable = func_carrega_dados_clientes,\n",
        "        provide_context = True,\n",
        "        dag = dag_log_solutions\n",
        "    )\n"
      ],
      "metadata": {
        "id": "bkYufMXVXBnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Comentando os tópicos da Função**\n",
        "\n"
      ],
      "metadata": {
        "id": "uheSjRlUX0W6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Obter o caminho do arquivo CSV**"
      ],
      "metadata": {
        "id": "FkB9ywtckpPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func_carrega_dados_clientes():\n",
        "\n",
        "    # Obter o caminho do arquivo CSV\n",
        "    csv_file_path = '/opt/airflow/dags/dados/DIM_CLIENTE.csv'\n"
      ],
      "metadata": {
        "id": "wZzMZnBxYGMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* **| def func_carrega_dados_clientes() | :** Esta linha define uma função chamada **func_carrega_dados_clientes** sem parâmetros.\n",
        "\n",
        "* **| csv_file_path = '/ opt/airflow/dags/dados/DIM_CLIENTE.csv' | :** Esta linha cria uma variável chamada **csv_file_path** que armazena uma string contendo o caminho do arquivo CSV.\\\n",
        "caminho do arquivo é **'/ opt/airflow/dags/dados/DIM_CLIENTE.csv'**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ctmp_kGri46D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Inicializar o contador**"
      ],
      "metadata": {
        "id": "tV1mCNyQktx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa o contador\n",
        "    i = 0"
      ],
      "metadata": {
        "id": "3E-f4aw0kMXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **| i = 0 | :** Isso inicializa uma variável chamada \"i\" com o valor 0. Essa variável será usada como um contador para contar o número de linhas no arquivo CSV.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5Fets54gloBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Abrir o Arquivo CSV e Incrementar um contador**"
      ],
      "metadata": {
        "id": "CayCnuuHnGa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Abrir o Arquivo CSV\n",
        "    with open(csv_file_path, 'r') as f:\n",
        "\n",
        "        reader = csv.DictReader(f)\n",
        "\n",
        "        for item in reader:\n",
        "\n",
        "            # Icrementa o contador\n",
        "            i += 1"
      ],
      "metadata": {
        "id": "2P87XlJ5nNWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **| with open(csv_file_path, 'r') as f| :** Esta linha abre um arquivo CSV especificado pelo caminho **csv_file_path** em modo de leitura **('r')**. O uso do bloco **with** garante que o arquivo seja fechado automaticamente após o término do processamento. O arquivo aberto é representado pela variável **f**.\n",
        "\n",
        "* **| reader = csv.DictReader(f) | :** Aqui, é criado um objeto **DictReader** do módulo **csv**. O objetivo deste objeto é ler as linhas do arquivo CSV e tratar a primeira linha como cabeçalho, onde os nomes das colunas são usados como chaves de dicionário. O objeto **DictReader** é inicializado com o arquivo **f**, permitindo que ele leia o conteúdo do arquivo.\n",
        "\n",
        "* **| for item in reader | :** Este é um **loop for** que itera sobre cada linha do **arquivo CSV**. A cada iteração, item conterá uma linha do **arquivo CSV** representada como um dicionário, onde as chaves do dicionário são os nomes das colunas e os valores são os dados daquela linha.\n",
        "\n",
        "* **| i += 1 | :** Dentro do **loop for**, esta linha incrementa o contador **\"i\"** em 1 a cada iteração. O **contador \"i\"** é usado para acompanhar o número de linhas **(registros)** no arquivo CSV. Cada vez que uma nova linha é lida, **\"i\"** é incrementado, o que permite contar quantas linhas existem no arquivo.\n",
        "\n",
        "**Obs: No geral, esse código tem a finalidade:**\n",
        "* Abrir um arquivo CSV;\n",
        "* ler cada linha do arquivo;\n",
        "* Contar o número total de linhas (registros) no arquivo e armazenar esse número na variável \"i\".\n",
        "* Isso pode ser útil para obter informações sobre o tamanho do arquivo ou para rastrear o progresso de um processo que envolve a leitura de um arquivo CSV.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FXv_OjvVoACL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Criando um novo dicionário**"
      ],
      "metadata": {
        "id": "fpXBJ8SkAPHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando um novo  dicionário\n",
        "            dados_cli = dict(item)"
      ],
      "metadata": {
        "id": "nUhAeZ6nAgTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O comando **dados_cli = dict(item)** serve para converter um dicionário Python em outro dicionário. Mais especificamente, ele cria uma cópia independente do dicionário representado pela variável **item** e armazena essa cópia na variável **dados_cli**.\n",
        "\n",
        "Aqui está o que cada parte do comando faz:\n",
        "\n",
        "* **| dict(item) |** : Esta parte do comando cria um novo dicionário utilizando a **função dict()**. O argumento passado para **dict()** é a variável **item**, que é um **dicionário existente**. Isso efetivamente cria uma cópia do dicionário item.\n",
        "\n",
        "* **| dados_cli |=** : Esta parte do comando atribui a cópia do dicionário à variável dados_cli. Portanto, após a execução dessa linha, a variável **dados_cli** contém uma cópia independente dos dados contidos em item.\n",
        "\n",
        "Essa cópia pode ser útil para fazer alterações nos dados sem afetar o dicionário original **item**. Em essência, isso permite trabalhar com duas versões separadas dos dados, uma em **item** e outra em **dados_cli**, facilitando a manipulação e o processamento de informações em seu código.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iBxjIMTrAjIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Inserir dados na tabela PostgreSQL**"
      ],
      "metadata": {
        "id": "On9le0kkD-Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " sql_query_cli =\n",
        "\n",
        " \"INSERT INTO varejo.DIM_CLIENTE (%s) VALUES (%s)\"\n",
        "\n",
        " % (','.join(dados_cli.keys()),\n",
        "\n",
        " ','.join([item for item in dados_cli.values()]))\n",
        "\n"
      ],
      "metadata": {
        "id": "dbbhGMnDEJIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **| sql_query_cli | :** Indica uma variável que armazena a **consulta SQL** que será executada posteriormente em um banco de dados. A consulta é usada para inserir dados em uma tabela chamada **varejo.DIM_CLIENTE**.\n",
        "\n",
        "\n",
        "* **| INSERT INTO varejo.DIM_CLIENTE (%s) VALUES (%s)\" :** Esta parte da string é a estrutura básica de uma consulta **SQL de inserção**. Ela começa com a instrução **SQL INSERT INTO**, seguida pelo nome do schema e tabela na qual os dados serão inseridos, que é **varejo.DIM_CLIENTE**. Em seguida, há dois marcadores de espaço reservado **%s** que serão substituídos pelos valores reais.\n",
        "\n",
        "* **| ' , '.join(dados_cli.keys()) |**:\n",
        "* **dados_cli.keys**() : retorna uma lista das chaves (nomes das colunas) de um dicionário chamado dados_cli. Essas chaves correspondem às colunas da tabela varejo.DIM_CLIENTE.\n",
        "\n",
        "* ' , '.**join**(...) : é usado para unir essas chaves em uma única string, separando-as por vírgulas. Isso cria a lista de nomes de colunas na consulta SQL. Por exemplo, se as chaves forem ['nome', 'idade'], isso se tornará \"nome,idade\".\n",
        "','.join([item for item in dados_cli.values()]):\n",
        "\n",
        "* **dados_cli.values()** : Retorna uma lista dos valores associados às chaves no dicionário dados_cli. Esses valores são os dados que serão inseridos nas colunas correspondentes da tabela.\n",
        "\n",
        "* **[item for item in dados_cli.values()]** : É uma compreensão de lista que cria uma lista dos valores do dicionário dados_cli. Por exemplo, se os valores forem ['João', 30], isso permanecerá igual: ['João', 30].\n",
        "\n",
        "* **' ,'.join(...)** : É usado novamente para unir esses valores em uma única string, separando-os por vírgulas. Isso cria a lista de valores que corresponderão às colunas na consulta SQL. Por exemplo, se os valores forem ['João', 30], isso se tornará \"'João',30\".\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0ZOXxPdVU0AY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico:  Operador do Postgres com incremento no id da tarefa (para cada linha inserida)**"
      ],
      "metadata": {
        "id": "LF-4b6A9iG7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)\n",
        "            postgres_operator = PostgresOperator(task_id = 'carrega_dados_clientes_' + str(i),\n",
        "                                                 sql = sql_query_cli,\n",
        "                                                 params = (dados_cli),\n",
        "                                                 postgres_conn_id = 'LOGDW',\n",
        "                                                 dag = dag_log_solutions)\n",
        "\n"
      ],
      "metadata": {
        "id": "Er4zqgtRh_y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **| postgres_operator = PostgresOperator(...) | :** Aqui, estamos criando uma instância de PostgresOperator e atribuindo-a à variável **postgres_operator**. Esta instância representa uma tarefa que será executada no fluxo de trabalho do **Apache Airflow.**\n",
        "\n",
        "* **|task_id = 'carrega_dados_clientes_' + str(i) | :** O parâmetro **task_id** é um identificador exclusivo para a tarefa. Ele está sendo definido dinamicamente com base no valor da variável i. Isso significa que cada tarefa terá um identificador único, com **\"carrega_dados_clientes_\"** seguido pelo valor de **i** convertido em string.\n",
        "\n",
        "* **| sql = sql_query_cli | :** O parâmetro sql contém a consulta SQL que será executada pelo operador. sql_query_cli deve ser uma string que contém a consulta SQL que extrairá dados da fonte de dados.\n",
        "\n",
        "* **| params = (dados_cli) | :** O parâmetro params permite passar parâmetros para a consulta SQL. dados_cli é uma variável que contém os parâmetros a serem usados na consulta SQL. Esses parâmetros podem ser usados para filtrar ou personalizar a consulta.\n",
        "\n",
        "* **| postgres_conn_id = 'LOGDW' | :** O parâmetro postgres_conn_id especifica a conexão com o banco de dados PostgreSQL que será usada para executar a consulta. Nesse caso, a conexão é identificada pelo nome \"LOGDW\", que deve estar configurado nas conexões do Airflow.\n",
        "\n",
        "* **| dag = dag_log_solutions | :** O parâmetro dag especifica a qual fluxo de trabalho (DAG - Directed Acyclic Graph) essa tarefa pertencerá. As tarefas fazem parte de um fluxo de trabalho maior, e dag_log_solutions é a DAG à qual essa tarefa será adicionada.\n",
        "\n",
        "Resumindo, o código cria uma tarefa que utiliza o **operador PostgresOperator** para executar uma **consulta SQL no banco de dados PostgreSQL.** A consulta é definida pela variável **sql_query_cli**, e os parâmetros são passados por meio da variável **dados_cli**. Cada tarefa tem um identificador exclusivo baseado no valor de **i** e faz parte da **DAG denominada dag_log_solutions**. Essa estrutura é usada para automatizar a carga de dados do **PostgreSQL** como parte de um fluxo de trabalho no **Apache Airflow.**\n",
        "___\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S_LJtAfdkr-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Executar o operador**"
      ],
      "metadata": {
        "id": "6AWWnU5SoJTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Executa o operador\n",
        "postgres_operator.execute()"
      ],
      "metadata": {
        "id": "U-w8qsJ3oXW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código executa o método **execute()** em uma instância do operador **PostgresOperator.**\n",
        "\n",
        "* **| postgres_operator |**:\\\n",
        "É uma instância do operador **PostgresOperator**, que provavelmente foi criada anteriormente no código. Este operador representa uma tarefa que executará uma consulta SQL em um banco de dados **PostgreSQL.**\n",
        "\n",
        "* **| execute() |**:\\\n",
        "É um método da classe **PostgresOperator** ou de uma classe pai na hierarquia. Quando este método é chamado, ele inicia a execução da tarefa representada pelo **operador**. Em outras palavras, **ele dispara a execução da consulta SQL definida na tarefa.**\n",
        "\n",
        "Portanto, ao chamar **postgres_operator.execute()**, o código está instruindo o **Apache Airflow** para iniciar a execução da tarefa representada por **postgres_operator**. Isso fará com que a consulta SQL seja enviada ao banco de dados **PostgreSQL** especificado no operador, e a tarefa será executada conforme definido no **DAG (Directed Acyclic Graph)** ao qual a tarefa pertence.\n",
        "\n",
        "Em resumo, essa linha de código é responsável por acionar a execução da tarefa que contém a **consulta SQL** no contexto de um fluxo de trabalho gerenciado pelo **Apache Airflow.**\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H7xlpRU6oyKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tópico: Carregar os dados na tabela Cliente**"
      ],
      "metadata": {
        "id": "1G396xjrpk4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tarefa_carrega_dados_clientes = PythonOperator(\n",
        "        task_id = 'tarefa_carrega_dados_clientes',\n",
        "        python_callable = func_carrega_dados_clientes,\n",
        "        provide_context = True,\n",
        "        dag = dag_log_solutions\n",
        "    )\n"
      ],
      "metadata": {
        "id": "his5yeA3pt2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código cria uma instância da **classe PythonOperator**, que é usada no **Apache Airflow** para definir tarefas que executarão código Python.\n",
        "\n",
        "* **|tarefa_carrega_dados_clientes | :** Isso é uma variável que representa a tarefa que está sendo definida. O nome **tarefa_carrega_dados_clientes** é um identificador único para essa tarefa dentro do **DAG (Directed Acyclic Graph)**. Você usará esse identificador para referenciar e controlar essa tarefa em outras partes do código.\n",
        "\n",
        "* **| task_id |** : É um parâmetro obrigatório e é o identificador único da tarefa dentro do DAG. No exemplo, o task_id é definido como **'tarefa_carrega_dados_clientes'**, o que significa que esta tarefa será conhecida pelo nome **'tarefa_carrega_dados_clientes'.**\n",
        "\n",
        "* **| python_callable |** : É um parâmetro que especifica a **função Python** que será executada quando a tarefa for acionada. Neste caso, **func_carrega_dados_clientes** é a função Python que será chamada. Isso significa que quando essa tarefa for executada, o código contido em **func_carrega_dados_clientes** será executado.\n",
        "\n",
        "* **|provide_context |** : É um parâmetro opcional que determina se o contexto do **Airflow** deve ser fornecido como argumento para a função Python. Se **provide_context** for definido como **True**, o contexto do **Airflow**, que inclui informações sobre a execução da tarefa, será passado como um dicionário para a função **func_carrega_dados_clientes**. Isso permite que a função Python acesse informações sobre a execução, como a data de início, data de término, variáveis definidas globalmente e muito mais.\n",
        "\n",
        "* **|dag |** : É o DAG ao qual esta tarefa pertence. Um DAG é um fluxo de trabalho composto por tarefas interconectadas. A tarefa que está sendo definida aqui faz parte do DAG chamado **dag_log_solutions.**\n",
        "\n",
        "Em resumo, o código cria uma tarefa chamada **'tarefa_carrega_dados_clientes'** que executa a função Python **func_carrega_dados_clientes** e tem acesso ao contexto do **Airflow.** Essa tarefa pode ser adicionada a um fluxo de trabalho (DAG) e programada para ser executada conforme necessário dentro do fluxo de trabalho.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "e0-l_hdypxM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Observação Importante**:\n",
        "Os dados que vão compor a tabela **Fato** e as dimensções abaixo\n",
        "* **Transportadoras**;\n",
        "* **Depósitos**;\n",
        "* **Entregas**;\n",
        "* **Frete**;\n",
        "* **Pagamentos**\n",
        "* **Data**\n",
        "\n",
        "Passaram pelo mesmo processo acima de criação dos codigos, ou seja o mesmo processo de criação do processo ETL - Extração, Transformação e carregamento desses dados que foi submedido os dados que vão compor a tebala cliente do DW, foram aplicados aos dados das demais dimensões e Fato que serão carregados no DW.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "oE98pwxbreUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kkzHmaT2vL3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tarefa_trunca_tb_fato = PostgresOperator(task_id = 'tarefa_trunca_tb_fato', postgres_conn_id = 'LOGDW',\\\n",
        "sql = \"TRUNCATE TABLE varejo.TB_FATO CASCADE\", dag = dag_log_solutions)"
      ],
      "metadata": {
        "id": "NJgkUzZXChxa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}